# Project Configuration
project:
  name: "LLM Medical Fine-tuning"
  version: "1.2.0"  # Updated after analysis
  author: "Nguyen Xuan Thanh"

# Model Configuration
model:
  name: "Qwen/Qwen2.5-1.5B-Instruct"
  quantization:
    enabled: true
    method: "4bit"
  max_length: 512
  max_new_tokens: 256

# Dataset Configuration
dataset:
  name: "ruslanmv/ai-medical-chatbot"
  max_samples: 5000  # CHANGED: Increased to 5000 to improve LoRA performance
  train_ratio: 0.9
  val_ratio: 0.1
  shuffle: true
  seed: 42
  instruction_template: |
    <|im_start|>system
    You are a helpful medical assistant.<|im_end|>
    <|im_start|>user
    {question}<|im_end|>
    <|im_start|>assistant
    {answer}<|im_end|>

# LoRA Configuration (Default) - OPTIMIZED v2 based on new experiments
lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.2  # INCREASED: Prevent overfitting observed after step 300
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  bias: "none"
  task_type: "CAUSAL_LM"

# Training Configuration - OPTIMIZED v2 based on analysis
training:
  output_dir: "./outputs"
  num_train_epochs: 3  # REDUCED: To match requirements (1-3 epochs)
  per_device_train_batch_size: 4  # INCREASED: Better gradient estimation
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 8  # Effective batch = 4*8 = 32
  learning_rate: 0.0002  # INCREASED: 2e-4 to match requirement range (2e-4 to 5e-4)
  weight_decay: 0.1  # INCREASED: Stronger regularization
  warmup_ratio: 0.1
  warmup_steps: 0
  lr_scheduler_type: "cosine"  # CHANGED: Simple cosine, restarts not needed
  neftune_noise_alpha: 5
  logging_steps: 10
  save_strategy: "steps"
  save_steps: 50
  save_total_limit: 3
  evaluation_strategy: "steps"
  eval_steps: 25  # MORE FREQUENT: Catch best model earlier
  fp16: false
  bf16: true
  optim: "paged_adamw_8bit"
  resume_from_checkpoint: false
  max_grad_norm: 1.0  # RELAXED: 0.5 was too aggressive
  
  # Early Stopping - TIGHTENED
  early_stopping:
    enabled: true
    patience: 4  # REDUCED: Stop earlier to prevent overfitting
    threshold: 0.001  # INCREASED: Less sensitive, avoid premature stopping

# Experiments Configuration - v3 EXPERIMENTS
# Experiments Configuration
experiments:
  # Base experiment
  - name: "lora_r16_alpha32"
    lora:
      r: 16
      lora_alpha: 32
      lora_dropout: 0.2
  
  # Comparison: Small Dataset (1k samples)
  - name: "experiment_samples_1k"
    lora:
      r: 16
      lora_alpha: 32
      lora_dropout: 0.2
    dataset:
      max_samples: 1000
  
  # Comparison: Large Dataset (5k samples)
  - name: "experiment_samples_5k"
    lora:
      r: 16
      lora_alpha: 32
      lora_dropout: 0.2
    dataset:
      max_samples: 5000
  
  # Comparison: High Rank
  - name: "lora_r32_alpha64"
    lora:
      r: 32
      lora_alpha: 64
      lora_dropout: 0.25

# Evaluation Configuration
evaluation:
  test_questions:
    - "What are the symptoms of diabetes?"
    - "How to treat a common cold?"
    - "What causes high blood pressure?"
    - "What are the side effects of aspirin?"
    - "How to prevent heart disease?"
    - "What should I do if I have chest pain?"
    - "How can I lower my cholesterol naturally?"
    - "What are the early signs of cancer?"
